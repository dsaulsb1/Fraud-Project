{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-749911199113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdb_password\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mawswrangler\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "from config import db_password\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"aws.nw.bootcamp.0805\"\n",
    "s3_bucket_path = \"creditcard.csv\"\n",
    "raw_s3_path = f\"s3://{s3_bucket}/{s3_bucket_path}\"\n",
    "\n",
    "creditcard_df = wr.s3.read_csv(path=raw_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to postgres\n",
    "con = wr.postgresql.connect(\"my-glue-connection\")\n",
    "wr.postgresql.to_sql(df=creditcard_df, table=\"CreditCard_Transactions\", schema=\"public\", con=con)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to AWS PgAdmin to read data\n",
    "creditcard_df = wr.postgresql.read_sql_table(table=\"CreditCard_Transactions\", schema=\"public\", con=con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection string\n",
    "db_string = f\"postgresql://postgresql:{db_password}@127.0.0.1:5432/Machine_Learning_Projects\"\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to PgAdmin to read data\n",
    "creditcard_df = pd.read_sql_query('select * from \"CreditCard_Transactions\"',con=engine)\n",
    "creditcard_df= creditcard_df.drop(['index'], axis=1)\n",
    "creditcard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking size of data to determine if we need to import in chunks\n",
    "creditcard_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset dataframe for Amount and Time for visualization\n",
    "viz_df = creditcard_df[['Amount', 'Time', 'Class']]\n",
    "viz_df = DataFrame(viz_df, columns=['Amount', 'Time', 'Class'])\n",
    "viz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the visualization data to PgAdmin\n",
    "viz_df.to_sql(name='Visualization_Table', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "Now we read the data and try to understand the meaning of each of the features. The python module pandas provide us with the functions to read data. In the next step, we will read the data from our directory, and then we look at the first five and last five rows of the data using head() and tail() attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df.head().append(creditcard_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time is recorded in the number of seconds since the first transaction in the data set. Therefore, we can conclude that this data set includes all transactions recorded over the course of two days. The features was prepared using PCA and so the physical interpretation of individual features does not make sense. The only features which have not been transformed with PCA are ‘Time’ and ‘Amount’. Feature ‘Class’ is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Visualization\n",
    "Determine the relative proportion of valid and fraudulent credit card transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraudulent Transactions: \" + str(len(creditcard_df[creditcard_df[\"Class\"] == 1])))\n",
    "print(\"Valid Transactions: \" + str(len(creditcard_df[creditcard_df[\"Class\"] == 0])))\n",
    "print(\"Proportion of Fraudulent Transactions: \" + str(len(creditcard_df[creditcard_df[\"Class\"] == 1])/ creditcard_df.shape[0]))\n",
    "\n",
    "# Determine the number of Fraudulent transactions\n",
    "fraud_proportion = creditcard_df.copy()\n",
    "fraud_proportion[\" \"] = np.where(fraud_proportion[\"Class\"] == 1 ,  \"Fraud\", \"Genuine\")\n",
    "\n",
    "%matplotlib inline\n",
    "# plot chart\n",
    "plt.figure(figsize=(16,8))\n",
    "ax1 = plt.subplot(121, aspect='equal')\n",
    "fraud_proportion[\" \"].value_counts().plot(kind='pie',  ax=ax1, startangle=0, legend = False, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pie chart shows an imbalance in the data, with only 0.17% of the total cases being fraudulent. Next, we check if there is any difference between the number of valid transactions and fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describle the data\n",
    "creditcard_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make sense to evaluate the results of the description of the data since most of the variables are principal component. Next, we focus on the Time and Amount columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df[['Time','Amount']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amount variable is highly skewed, with 75% of all transactions below $77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)\n",
    "\n",
    "amount_val = creditcard_df['Amount'].values\n",
    "time_val = creditcard_df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, hist=False, color=\"c\", kde_kws={\"shade\": True}, ax=axes[0]).set_title('Distribution of Transaction Amount')\n",
    "sns.distplot(time_val, hist=False, color=\"c\", kde_kws={\"shade\": True}, ax=axes[1]).set_title('Distribution of Transaction Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Amount in a Fraudulent Transaction: \" + str(creditcard_df[creditcard_df[\"Class\"] == 1][\"Amount\"].mean()))\n",
    "print(\"Average Amount in a Valid Transaction: \" + str(creditcard_df[creditcard_df[\"Class\"] == 0][\"Amount\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average Amount for fraudulent transactions is higher than the average for valid transactions. Next, we will try to understand the distribution of values in each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Amount\n",
    "print(\"Summary of the feature - Amount\" + \"\\n-------------------------------\")\n",
    "print(creditcard_df[\"Amount\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next we look at the distribution of each feature [grouped by Class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_distplots(dataframe, features, rows, cols):\n",
    "    features = data_plot.iloc[:,0:30].columns\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax=fig.add_subplot(rows,cols,i+1)\n",
    "        sns.distplot(dataframe[feature][dataframe.Class == 1], hist=False, kde_kws={\"shade\": True}, bins=50)\n",
    "        sns.distplot(dataframe[feature][dataframe.Class == 0], hist=False, kde_kws={\"shade\": True}, bins=50)\n",
    "        #dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_title(\"Distribution of Column: \"  + str(feature))\n",
    "        #ax.set_yscale('log')\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "draw_distplots(data_plot,data_plot.columns,8,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bivariate plots show that most of the features are normally distributed for valid transaction class. Conversely, the Fraud Class shows a wider spread as expected. Next, we move to data preparation, where we would handle missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "\n",
    "Since we have a small number of features which are created using PCA,feature selection is not a necessary step. Next, we move on to handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of cases with non-missing values: \" + str(creditcard_df.isnull().shape[0]))\n",
    "print(\"Number of cases with missing values: \" + str(creditcard_df.shape[0] - creditcard_df.isnull().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have any missing data, the next step is to standardize the Time and Amount features using the RobustScaler. The choice of using the RobustScaler over the StandardScaler and the MinMaxScaler is the that the RobustScaler reduces the effects of outliers, relative to the MinMaxScaler. It is important to that many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed. That is why we are taking this scaling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler().fit(creditcard_df[[\"Time\", \"Amount\"]])\n",
    "creditcard_df[[\"Time\", \"Amount\"]] = scaler.transform(creditcard_df[[\"Time\", \"Amount\"]])\n",
    "\n",
    "creditcard_df.head().append(creditcard_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the scaled data to PgAdmin\n",
    "db_string = f\"postgresql://postgresql:{db_password}@127.0.0.1:5432/Machine_Learning_Projects\"\n",
    "engine = create_engine(db_string)\n",
    "creditcard_df.to_sql(name='CreditCard_Transactions_Scaled', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to PgAdmin to read the scaled_data\n",
    "creditcard_scaled_df = pd.read_sql_query('select * from \"CreditCard_Transactions_Scaled\"',con=engine)\n",
    "creditcard_scaled_df= creditcard_scaled_df.drop(['index'], axis=1)\n",
    "creditcard_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to postgres\n",
    "con = wr.postgresql.connect(\"my-glue-connection\")\n",
    "wr.postgresql.to_sql(df=creditcard_df, table=\"CreditCard_Transactions_Scaled\", schema=\"public\", con=con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# connect to AWS PgAdmin to read data\n",
    "creditcard_scaled_df = wr.postgresql.read_sql_table(table=\"CreditCard_Transactions_Scaled\", schema=\"public\", con=con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier Detection\n",
    "\n",
    "Outlier handling depends on the type of problem we are trying to solve. In a balanced dataset, it makes to remove outliers since they could potentially affect our model. In this classification problem, our dataset is highly imbalanced amd we are trying to detect the outlier transactions, hence it makes sense that we do not remove the outliers found in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling\n",
    "First we divide the data into response and features. And also make the train-test split of the data for further modelling and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate response and features\n",
    "y = creditcard_scaled_df[\"Class\"]\n",
    "X = creditcard_scaled_df.iloc[:,0:30]\n",
    "\n",
    "# Split training/test datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we identified earlier, the dataset is highly imbalanced. Fitting a model on this dataset will result in overfitting towards the majority class. To illustrate, we run one model (Random Forest or logistic regression) on the imbalanced data and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "log_classifier = LogisticRegression(solver=\"lbfgs\",max_iter=200)\n",
    "log_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_lrc = log_classifier.predict(X_test)\n",
    "print(f\"The accuracy of the model is: {accuracy_score(y_test,y_pred_lrc):.4f}\")\n",
    "print(f\"The pecision of the model is: {precision_score(y_test,y_pred_lrc):.4f}\")\n",
    "print(f\"The recall of the model is: {recall_score(y_test,y_pred_lrc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_recall = recall_score(y_test, y_pred_lrc)\n",
    "logistic_accuracy = accuracy_score(y_test, y_pred_lrc)\n",
    "logistic_precision = precision_score(y_test, y_pred_lrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lrc = confusion_matrix(y_test, y_pred_lrc)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_lrc_df = pd.DataFrame(cm_lrc, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_lrc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While model accuracy is 100%, our classifier did not do an excellent job at predicting fraudulent transactions. With precision and recall of 0.84 and 0.62, we would need a better understanding of the dataset to determine the best preprocessing steps to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Define the random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Fit the model\n",
    "rfc = RandomForestClassifier() \n",
    "rfc.fit(X_train, y_train)\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "y_pred_rfc = rfc.predict(X_test)\n",
    "print(f\"The accuracy of the random forest model is: {accuracy_score(y_test,y_pred_rfc):.4f}\")\n",
    "print(f\"The pecision of the random forest model is: {precision_score(y_test,y_pred_rfc):.4f}\")\n",
    "print(f\"The recall of the random forest model is: {recall_score(y_test,y_pred_rfc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_recall = recall_score(y_test, y_pred_rfc)\n",
    "rfc_accuracy = accuracy_score(y_test, y_pred_rfc)\n",
    "rfc_precision = precision_score(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#y_pred_rfc = naive_rfc.predict(X_test)\n",
    "cm_rfc = confusion_matrix(y_test, y_pred_rfc)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_rfc_df = pd.DataFrame(cm_rfc, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_rfc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the scores\n",
    "naive_data_score = [['Logistic Regression', logistic_accuracy, logistic_recall, logistic_precision], \n",
    "        ['Random Forest', rfc_accuracy, rfc_recall, rfc_precision] ] \n",
    "  \n",
    "# Create the dataframe \n",
    "naive_data_table = pd.DataFrame(naive_data_score, columns = ['Classifier', 'Accuracy', 'Recall Score', 'Precision Score']) \n",
    "naive_data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While model accuracy is 100%, and precision is 95%, our random forest classifier only achieved a 77% recall. We would need a better understanding of the dataset to determine the best preprocessing steps to take.\n",
    "One thing to notice here is, we had only 0.17% cases with fraud transactions and a model predicting all trasactions to be valid would have similar accuracy. So we need to train our model in a way that is not overfitted to either of the classes. For this, we introduce Oversampling and Undersampling methods. Oversampling resamples from the minority class to balance the class proportions, and undersampling merges or removes similar observations from the majority to achive the same.\n",
    "\n",
    "Undersampling\n",
    "In this section we first describe the structure of the modelling and validations. One trivial point to note is, we will not undersample the test data as we want our model to perform well with skewed class distributions eventually. The steps are as follows (The whole set-up will be structured using the imbalance-learn module):\n",
    "\n",
    "Use a 5-fold cross validation on the training set\n",
    "On each of the folds use undersampling\n",
    "Fit the model on the training folds and validate on the validation fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cross validation framework \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "kf = StratifiedKFold(n_splits=2, random_state = 42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the imbalance Learn module\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Logistic Regression \n",
    "imba_pipeline = make_pipeline(NearMiss(), LogisticRegression())\n",
    "\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [ 0.01, 0.1, 1, 100], 'solver' : ['liblinear']}\n",
    "new_params = {'logisticregression__' + key: log_reg_params[key] for key in log_reg_params}\n",
    "\n",
    "grid_imba_log_reg = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, n_jobs=-1, return_train_score=True)\n",
    "grid_imba_log_reg.fit(X_train, y_train);\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logistic_cv_score_us = cross_val_score(grid_imba_log_reg, X_train, y_train, scoring = 'recall', cv = kf, n_jobs=-1)\n",
    "\n",
    "pred_log_reg_us = grid_imba_log_reg.best_estimator_.named_steps['logisticregression'].predict(X_test)\n",
    "logistic_recall_us = recall_score(y_test, pred_log_reg_us)\n",
    "logistic_accuracy_us = accuracy_score(y_test, pred_log_reg_us)\n",
    "logistic_precision_us = precision_score(y_test, pred_log_reg_us)\n",
    "\n",
    "log_reg_us = grid_imba_log_reg.best_estimator_\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_log_reg_us = confusion_matrix(y_test, pred_log_reg_us)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_log_reg_us_df = pd.DataFrame(cm_pred_log_reg_us, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_log_reg_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the logistic regression-us model is: {logistic_accuracy_us:.4f}\")\n",
    "print(f\"The pecision of the logistic regression-us model is: {logistic_precision_us:.4f}\")\n",
    "print(f\"The recall of the logistic regression-us model is: {logistic_recall_us:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_us, logistic_cv_score_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "yproba = grid_imba_log_reg.best_estimator_.named_steps['logisticregression'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"Logistic Regression\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Define the pipeline\n",
    "imba_pipeline = make_pipeline(NearMiss(), RandomForestClassifier())\n",
    "\n",
    "params = {'n_estimators': [50, 100, 200], 'max_depth': [4, 6, 10, 12], 'random_state': [13] }\n",
    "new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "\n",
    "grid_imba_rf = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, n_jobs=-1, return_train_score=True)\n",
    "grid_imba_rf.fit(X_train, y_train);\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "rfc_cv_score_us = cross_val_score(grid_imba_rf, X_train, y_train, scoring='recall', cv=kf, n_jobs=-1)\n",
    "\n",
    "pred_rfc_us = grid_imba_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)\n",
    "rfc_precision_us = precision_score(y_test, pred_rfc_us)\n",
    "rfc_recall_us = recall_score(y_test, pred_rfc_us)\n",
    "rfc_accuracy_us = accuracy_score(y_test, pred_rfc_us)\n",
    "\n",
    "rfc_us = grid_imba_rf.best_estimator_\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_rfc_us = confusion_matrix(y_test, pred_rfc_us)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_rfc_us_df = pd.DataFrame(cm_pred_rfc_us, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_rfc_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the random forest-us classifier is: {rfc_accuracy_us:.4f}\")\n",
    "print(f\"The pecision of the random forest-us classifier is: {rfc_precision_us:.4f}\")\n",
    "print(f\"The recall of the random forest-us classifier is: {rfc_recall_us:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_us, rfc_cv_score_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "yproba = grid_imba_rf.best_estimator_.named_steps['randomforestclassifier'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"Random Forest\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling - Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Define the pipeline\n",
    "imba_pipeline = make_pipeline(NearMiss(), SVC(probability = True))\n",
    "\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "new_params = {'svc__' + key: svc_params[key] for key in svc_params}\n",
    "\n",
    "grid_imba_svc = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, n_jobs=-1, return_train_score=True)\n",
    "grid_imba_svc.fit(X_train, y_train);\n",
    "\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "svc_cv_score_us = cross_val_score(grid_imba_svc, X_train, y_train, scoring='recall', cv=kf, n_jobs=-1)\n",
    "\n",
    "pred_svc_us = grid_imba_svc.best_estimator_.named_steps['svc'].predict(X_test)\n",
    "svc_recall_us = recall_score(y_test, pred_svc_us)\n",
    "svc_accuracy_us = accuracy_score(y_test, pred_svc_us)\n",
    "svc_precision_us = precision_score(y_test, pred_svc_us)\n",
    "\n",
    "svc_us = grid_imba_svc.best_estimator_\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_svc_us = confusion_matrix(y_test, pred_svc_us)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_svc_us_df = pd.DataFrame(cm_pred_svc_us, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_svc_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the support vector - undersampling model is: {svc_accuracy_us:.4f}\")\n",
    "print(f\"The pecision of the support vector - undersampling model is: {svc_precision_us:.4f}\")\n",
    "print(f\"The recall of the support vector - undersampling model is: {svc_recall_us:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_us, svc_cv_score_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "yproba = grid_imba_svc.best_estimator_.named_steps['svc'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"Support Vector Classifier\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# DecisionTree Classifier\n",
    "imba_pipeline = make_pipeline(NearMiss(),  DecisionTreeClassifier())\n",
    "\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\n",
    "new_params = {'decisiontreeclassifier__' + key: tree_params[key] for key in tree_params}\n",
    "\n",
    "grid_imba_tree = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, n_jobs=-1, return_train_score=True)\n",
    "grid_imba_tree.fit(X_train, y_train);\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "dtree_cv_score_us = cross_val_score(grid_imba_tree, X_train, y_train, scoring='recall', cv=kf, n_jobs=-1)\n",
    "\n",
    "pred_dtc_us = grid_imba_tree.best_estimator_.named_steps['decisiontreeclassifier'].predict(X_test)\n",
    "dtree_recall_us = recall_score(y_test, pred_dtc_us)\n",
    "dtree_accuracy_us = accuracy_score(y_test, pred_dtc_us)\n",
    "dtree_precision_us = precision_score(y_test, pred_dtc_us)\n",
    "\n",
    "dtc_us = grid_imba_tree.best_estimator_\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_dtc_us = confusion_matrix(y_test, pred_dtc_us)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_dtc_us_df = pd.DataFrame(cm_pred_dtc_us, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_dtc_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the decision tree-us classifier is: {dtree_accuracy_us:.4f}\")\n",
    "print(f\"The pecision of the decision tree-us classifier is: {dtree_precision_us:.4f}\")\n",
    "print(f\"The recall of the decision tree-us classifier is: {dtree_recall_us:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_us, dtree_cv_score_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "yproba = grid_imba_tree.best_estimator_.named_steps['decisiontreeclassifier'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"Decision Tree\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling - k-Nearest Neighbour Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# KNeighbors Classifier\n",
    "imba_pipeline = make_pipeline(NearMiss(), KNeighborsClassifier())\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "new_params = {'kneighborsclassifier__' + key: knears_params[key] for key in knears_params}\n",
    "\n",
    "grid_imba_knn = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, n_jobs=-1, return_train_score=True)\n",
    "grid_imba_knn.fit(X_train, y_train);\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "knear_cv_score_us = cross_val_score(grid_imba_knn, X_train, y_train, scoring='recall', cv=kf, n_jobs=-1)\n",
    "\n",
    "pred_knn_us = grid_imba_knn.best_estimator_.named_steps['kneighborsclassifier'].predict(X_test)\n",
    "knear_recall_us = recall_score(y_test, pred_knn_us)\n",
    "knear_accuracy_us = accuracy_score(y_test, pred_knn_us)\n",
    "knear_precision_us = precision_score(y_test, pred_knn_us)\n",
    "\n",
    "knn_us = grid_imba_knn.best_estimator_\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_knn_us = confusion_matrix(y_test, pred_knn_us)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_knn_us_df = pd.DataFrame(cm_pred_knn_us, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_knn_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the KNN - undersampling model is: {knear_accuracy_us:.4f}\")\n",
    "print(f\"The pecision of the KNN - undersampling model is: {knear_precision_us:.4f}\")\n",
    "print(f\"The recall of the KNN - undersampling model is: {knear_recall_us:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_us, knear_cv_score_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "yproba = grid_imba_knn.best_estimator_.named_steps['kneighborsclassifier'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"k-Nearest Neighbour\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the undersampling model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the scores\n",
    "data_score = [['Logistic Regression', logistic_cv_score_us.mean(), logistic_accuracy_us, logistic_recall_us, logistic_precision_us], \n",
    "        ['Random Forest', rfc_cv_score_us.mean(), rfc_accuracy_us, rfc_recall_us, rfc_precision_us], \n",
    "        ['Support Vector', svc_cv_score_us.mean(), svc_accuracy_us, svc_recall_us, svc_precision_us],\n",
    "        ['Decision Tree', dtree_cv_score_us.mean(), dtree_accuracy_us, dtree_recall_us, dtree_precision_us],\n",
    "        ['k-Nearest Neighbour', knear_cv_score_us.mean(), knear_accuracy_us, knear_recall_us, knear_precision_us]\n",
    "             ] \n",
    "  \n",
    "# Create the dataframe \n",
    "data_table = pd.DataFrame(data_score, columns = ['Classifier', 'CV Score', 'Accuracy', 'Recall Score', 'Precision Score']) \n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the ROC curve for the above classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for undersampling\n",
    "result_table.set_index('classifiers', inplace=True)\n",
    "fig = plt.figure(figsize=(17,7))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis for Undersampling', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The Learning Curve\n",
    "\n",
    "Here we choose 4 models and try to see the trend of training and cross-validation scores over varrying training size. A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set with varying sizes will be used to train the estimator and a score for each training subset size and the test set will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = creditcard_scaled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Plot LogisticRegression Learning Curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    # First Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = \"recall\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
    "    ax1.set_xlabel('Training size (m)')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc=\"best\")\n",
    "    \n",
    "    # Second Estimator \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = \"recall\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n",
    "    ax2.set_xlabel('Training size (m)')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    # Third Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = \"recall\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax3.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n",
    "    ax3.set_xlabel('Training size (m)')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend(loc=\"best\")\n",
    "    \n",
    "    # Fourth Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = \"recall\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax4.set_title(\"Random Forest Classifier \\n Learning Curve\", fontsize=14)\n",
    "    ax4.set_xlabel('Training size (m)')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.grid(True)\n",
    "    ax4.legend(loc=\"best\")\n",
    "    return plt\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "\n",
    "df = data.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "genuine_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, genuine_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "y = new_df[\"Class\"]\n",
    "X = new_df.iloc[:,0:30]\n",
    "%matplotlib inline\n",
    "plot_learning_curve(log_reg_us, knn_us, dtc_us, rfc_us, X, y, (0.8, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we first describe the structure of the modelling and validations. One trivial point to note is, we will not oversample the test data as we want our model to perform well with skewed class distributions eventually. The steps are as follows (The whole set-up will be structured using the imbalance-learn module):\n",
    "\n",
    "Use a 5-fold cross validation on the training set\n",
    "On each of the folds use oversampling\n",
    "Fit the model on the training folds and validate on the validation fold\n",
    "Note that we will use the best model parameters as obtained from grid-search algorithm in Undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42), LogisticRegression())\n",
    "\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [ 0.01, 0.1, 1, 100, 100], 'solver' : ['liblinear']}\n",
    "new_params = {'logisticregression__' + key: log_reg_params[key] for key in log_reg_params}\n",
    "\n",
    "ran_imba_log_reg = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, n_jobs=-1, scoring='recall', return_train_score=True)\n",
    "ran_imba_log_reg.fit(X_train, y_train);\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logistic_cv_score_os = cross_val_score(ran_imba_log_reg, X_train, y_train, scoring = 'recall', cv = kf, n_jobs=-1)\n",
    "\n",
    "pred_log_reg_os = ran_imba_log_reg.best_estimator_.named_steps['logisticregression'].predict(X_test)\n",
    "logistic_recall_os = recall_score(y_test, pred_log_reg_os)\n",
    "logistic_accuracy_os = accuracy_score(y_test, pred_log_reg_os)\n",
    "logistic_precision_os = precision_score(y_test, pred_log_reg_os)\n",
    "\n",
    "log_reg_os = ran_imba_log_reg.best_estimator_\n",
    "print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_log_reg_os = confusion_matrix(y_test, pred_log_reg_os)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_log_reg_os_df = pd.DataFrame(cm_log_reg_os, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_log_reg_os_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the logistric regression-os classifier is: {logistic_accuracy_os:.4f}\")\n",
    "print(f\"The pecision of the logistric regression-os classifier is: {logistic_precision_os:.4f}\")\n",
    "print(f\"The recall of the logistric regression-os classifier is: {logistic_recall_os:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_os, logistic_cv_score_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42), RandomForestClassifier())\n",
    "\n",
    "params = {'n_estimators': [50, 100, 200], 'max_depth': [4, 6, 10, 12], 'random_state': [13] }\n",
    "new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "\n",
    "ran_imba_rf = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, n_jobs=-1, scoring='recall', return_train_score=True)\n",
    "ran_imba_rf.fit(X_train, y_train);\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "\"max_depth\": [3, 4, 7, 10, 25],\n",
    "\"gamma\": [0.5, 1, 5, 10, 25],\n",
    "\"min_child_weight\": [1, 3, 5, 10, 25],\n",
    "\"reg_lambda\": [5, 10, 50, 100, 300],\n",
    "\"scale_pos_weight\": [1, 3, 5, 10, 25]\n",
    "}\n",
    "# Grid Search CV implementation\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "halving_cv = HalvingGridSearchCV(xgb_cl, param_grid, scoring=\"roc_auc\", n_jobs=-1, min_resources=\"exhaust\", factor=3)\n",
    "halving_cv.fit(X_train, y_train)\n",
    "# Return set of parameters with the best performance\n",
    "halving_cv.best_params_\n",
    "# Return the performance metric score\n",
    "halving_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "rf_cv_score_os = cross_val_score(ran_imba_rf, X_train, y_train, scoring = 'recall', cv = kf, n_jobs=-1)\n",
    "\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pred_rfc_os = ran_imba_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)\n",
    "rfc_recall_os = recall_score(y_test, pred_rfc_os)\n",
    "rfc_accuracy_os = accuracy_score(y_test, pred_rfc_os)\n",
    "rfc_precision_os = precision_score(y_test, pred_rfc_os)\n",
    "\n",
    "rfc_os = ran_imba_rf.best_estimator_\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_rfc_os = confusion_matrix(y_test, pred_rfc_os)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_rfc_os_df = pd.DataFrame(cm_pred_rfc_os, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_rfc_os_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the random forest - oversampling model is: {rfc_accuracy_os:.4f}\")\n",
    "print(f\"The pecision of the random forest - oversampling model is: {rfc_precision_os:.4f}\")\n",
    "print(f\"The recall of the random forest - oversampling model is: {rfc_recall_os:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_os, rf_cv_score_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "yproba = ran_imba_rf.best_estimator_.named_steps['randomforestclassifier'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"Random Forest\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Oversampling - Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42), SVC())\n",
    "\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "new_params = {'svc__' + key: svc_params[key] for key in svc_params}\n",
    "ran_imba_svc = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, n_jobs=-1, scoring='recall', return_train_score=True)\n",
    "ran_imba_svc.fit(X_train, y_train);\n",
    "print(f'Done. {(time.time() - start_time)} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "ran_imba_svc.fit(X_train, y_train);\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "svc_cv_score_os = cross_val_score(ran_imba_svc, X_train, y_train, scoring = 'recall', cv = kf, n_jobs=-1)\n",
    "\n",
    "pred_svc_os = ran_imba_svc.best_estimator_.named_steps['svc'].predict(X_test)\n",
    "svc_recall_os = recall_score(y_test, pred_svc_os)\n",
    "svc_accuracy_os = accuracy_score(y_test, pred_svc_os)\n",
    "svc_precision_os = precision_score(y_test, pred_svc_os)\n",
    "\n",
    "svc_os = ran_imba_svc.best_estimator_\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_svc_os = confusion_matrix(y_test, pred_svc_os)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_svc_os_df = pd.DataFrame(cm_pred_svc_os, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_svc_os_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the support vector-os classifier is: {svc_accuracy_os:.4f}\")\n",
    "print(f\"The pecision of the support vector-os classifier is: {svc_precision_os:.4f}\")\n",
    "print(f\"The recall of the support vector-os classifier is: {svc_recall_os:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_os, svc_cv_score_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Oversampling - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# DecisionTree Classifier\n",
    "imba_pipeline = make_pipeline(NearMiss(), DecisionTreeClassifier())\n",
    "\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\n",
    "new_params = {'decisiontreeclassifier__' + key: tree_params[key] for key in tree_params}\n",
    "\n",
    "grid_imba_tree = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, return_train_score=True)\n",
    "\n",
    "grid_imba_tree.fit(X_train, y_train);\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "dtree_cv_score_us = cross_val_score(grid_imba_tree, X_train, y_train, scoring='recall', cv=kf)\n",
    "\n",
    "pred_dtc_os = grid_imba_tree.best_estimator_.named_steps['decisiontreeclassifier'].predict(X_test)\n",
    "dtree_recall_os = recall_score(y_test, pred_dtc_os)\n",
    "dtree_accuracy_os = accuracy_score(y_test, pred_dtc_os)\n",
    "dtree_precision_os = precision_score(y_test, pred_dtc_os)\n",
    "\n",
    "tree_clf_os = grid_imba_tree.best_estimator_\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_dtc_os = confusion_matrix(y_test, pred_dtc_os)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_dtc_os_df = pd.DataFrame(cm_pred_dtc_os, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_dtc_os_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the decision tree - oversampling model is: {dtree_accuracy_os:.4f}\")\n",
    "print(f\"The pecision of the decision tree - oversampling model is: {dtree_precision_os:.4f}\")\n",
    "print(f\"The recall of the decision tree - oversampling model is: {dtree_recall_os:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_os, dtree_cv_score_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "yproba = grid_imba_tree.best_estimator_.named_steps['decisiontreeclassifier'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"Decision Tree Classifier\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling - K-Nearest Neighbour Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42), KNeighborsClassifier())\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "new_params = {'kneighborsclassifier__' + key: knears_params[key] for key in knears_params}\n",
    "\n",
    "ran_imba_knn = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, n_jobs=-1, scoring='recall', return_train_score=True)\n",
    "ran_imba_knn.fit(X_train, y_train);\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "knear_cv_score_os = cross_val_score(ran_imba_knn, X_train, y_train, scoring = 'recall', cv = kf, n_jobs=-1)\n",
    "\n",
    "pred_knn_os = ran_imba_knn.best_estimator_.named_steps['kneighborsclassifier'].predict(X_test)\n",
    "knn_recall_os = recall_score(y_test, pred_knn_os)\n",
    "knn_precision_os = precision_score(y_test, pred_knn_os)\n",
    "knn_accuracy_os = accuracy_score(y_test, pred_knn_os)\n",
    "\n",
    "knn_os = ran_imba_knn.best_estimator_\n",
    "print(f'Done. {(time.time() - start_time)/60.0} total minutes elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pred_knn_os = confusion_matrix(y_test, pred_knn_os)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_pred_knn_os_df = pd.DataFrame(cm_pred_knn_os, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "cm_pred_knn_os_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the knn - oversampling model is: {knn_accuracy_os:.4f}\")\n",
    "print(f\"The pecision of the knn - oversampling model is: {knn_precision_os:.4f}\")\n",
    "print(f\"The recall of the knn - oversampling model is: {knn_recall_os:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_os, knear_cv_score_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "yproba = grid_imba_tree.best_estimator_.named_steps['kneighborsclassifier'].predict_proba(X_test)[::,1]\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "\n",
    "result_table = result_table.append({'classifiers': \"k-Nearest Neighbour\",\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we summarize all the recall scores in a table for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the scores\n",
    "final_scores = [['Logistic Regression', logistic_accuracy_us, logistic_accuracy_os, logistic_recall_us, logistic_recall_os], \n",
    "        ['Random Forest', rfc_accuracy_us, rfc_accuracy_os, rfc_recall_us, rfc_recall_os], \n",
    "        ['Support Vector', svc_accuracy_us, svc_accuracy_os, svc_recall_us, svc_recall_os],\n",
    "        ['Decision Tree', dtree_accuracy_us, dtree_accuracy_os, dtree_recall_us, dtree_recall_os],\n",
    "        ['k-Nearest Neighbour', knear_recall_us, knear_recall_os, knear_recall_us, knear_recall_os]\n",
    "             ] \n",
    "  \n",
    "# Create the dataframe \n",
    "final_df = pd.DataFrame(final_scores, columns = ['Classifier', 'Accuracy - Random UnderSampling', 'Accuracy - Oversampling (SMOTE)',\n",
    "                                                'Recall - Random UnderSampling', 'Recall - Oversampling (SMOTE)']) \n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the scores\n",
    "final_scores = [['Logistic Regression', logistic_accuracy, logistic_recall, logistic_accuracy_us, logistic_accuracy_os, logistic_recall_us, logistic_recall_os], \n",
    "        ['Random Forest', rfc_accuracy, rfc_recall, rfc_accuracy_us, rfc_accuracy_os, rfc_recall_us, rfc_recall_os], \n",
    "        ['Decision Tree', 'Not Applicable', 'Not Applicable',dtree_accuracy_us, dtree_accuracy_os, dtree_recall_us, dtree_recall_os]\n",
    "        ] \n",
    "  \n",
    "# Create the dataframe \n",
    "final_df = pd.DataFrame(final_scores, columns = ['Classifier', 'Naive - Accuracy', 'Naive - Recall', 'Accuracy - Random UnderSampling', 'Accuracy - Oversampling (SMOTE)',\n",
    "                                                'Recall - Random UnderSampling', 'Recall - Oversampling (SMOTE)']) \n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curve for Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for undersampling\n",
    "result_table.set_index('classifiers', inplace=True)\n",
    "fig = plt.figure(figsize=(17,7))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis for Oversampling', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(log_reg_os, tree_clf_os, rfc_os, X, y, (0.8, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please note that we did not use the outlier detection because sometimes we want the features in the model to have some extreme values to train the model accordingly. Also, this problem was an example of anomaly detection () and hence we did not want to get rid of the extreme values in features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
